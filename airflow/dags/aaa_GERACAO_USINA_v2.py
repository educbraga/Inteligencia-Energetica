from __future__ import annotations

"""
DAG para cria√ß√£o autom√°tica de sources e connections no Airbyte para dados GERACAO_USINA_2

VARI√ÅVEIS NECESS√ÅRIAS NO AIRFLOW:
- AIRBYTE_WORKSPACE_ID: ID do workspace do Airbyte
- AIRBYTE_CLIENT_ID: Client ID da aplica√ß√£o Airbyte  
- AIRBYTE_CLIENT_SECRET: Client Secret da aplica√ß√£o Airbyte
- AIRBYTE_DESTINATION_ID: ID do destino Snowflake no Airbyte

Para configurar as vari√°veis no Airflow:
1. Acesse Admin > Variables na UI do Airflow
2. Clique em "+" para criar uma nova vari√°vel
3. Adicione cada vari√°vel com seu respectivo valor

NOTA: Estas vari√°veis j√° devem estar configuradas se a v1 estiver funcionando.
"""

import requests
import json
from datetime import datetime, timedelta
from airflow.models.dag import DAG
from airflow.decorators import task
from airflow.models import Variable
from airflow.operators.python import get_current_context

dag = DAG(
    dag_id='aaa_GERACAO_USINA_2_v2',
    schedule=None,  # Execu√ß√£o √∫nica, n√£o recorrente
    start_date=datetime(2025, 1, 1),
    catchup=False,
    max_active_runs=1,
    tags=['airbyte', 'sources', 'historical', 'energy', 'geracao', 'yearly-monthly'],
    description='Criar sources no Airbyte para GERACAO_USINA_2 - v2 com l√≥gica para dados anuais (2000-2021) e mensais (2022+)',
    default_args={
        'owner': 'data_team',
        'depends_on_past': False,
        'email_on_failure': False,
        'email_on_retry': False,
        'retries': 1,
        'retry_delay': timedelta(minutes=1),
        'start_date': datetime(2025, 1, 1),
    },
    params={
        "start_date": "2025-01-01",  # Data de in√≠cio da captura (formato YYYY-MM-DD)
        "end_date": "auto",          # "auto" para at√© m√™s atual (exclui futuros) ou YYYY-MM-DD para data espec√≠fica
        "force_recreate": False,     # Se True, recria sources mesmo se j√° existirem
        "trigger_sync": True,        # Se True, inicia sincroniza√ß√£o autom√°tica ap√≥s criar connections
        "include_historical": True,  # Se True, inclui dados hist√≥ricos (2000-2021)
        "include_recent": True       # Se True, inclui dados recentes (2022+)
    }
)

def get_access_token():
    """Gera um novo access token usando client credentials"""
    try:
        client_id = Variable.get("AIRBYTE_CLIENT_ID", default_var=None)
        client_secret = Variable.get("AIRBYTE_CLIENT_SECRET", default_var=None)
        
        if not client_id or not client_secret:
            raise Exception("‚ùå Vari√°veis AIRBYTE_CLIENT_ID e AIRBYTE_CLIENT_SECRET devem ser configuradas no Airflow")
            
    except Exception as e:
        raise Exception(f"Erro ao obter vari√°veis do Airflow: {str(e)}")
    
    response = requests.post(
        "https://api.airbyte.com/v1/applications/token",
        headers={"Content-Type": "application/json"},
        json={
            "client_id": client_id,
            "client_secret": client_secret
        }
    )
    
    if response.status_code != 200:
        raise Exception(f"Erro ao obter access token: {response.status_code} - {response.text}")
    
    return response.json()["access_token"]

def make_api_request(method, endpoint, data=None):
    """Faz uma requisi√ß√£o √† API do Airbyte com token refresh autom√°tico"""
    access_token = get_access_token()
    base_url = "https://api.airbyte.com/v1"
    
    headers = {
        "Authorization": f"Bearer {access_token}",
        "Content-Type": "application/json"
    }
    
    url = f"{base_url}{endpoint}"
    
    if method.upper() == "GET":
        response = requests.get(url, headers=headers)
    elif method.upper() == "POST":
        response = requests.post(url, headers=headers, json=data)
    elif method.upper() == "DELETE":
        response = requests.delete(url, headers=headers)
    else:
        raise ValueError(f"M√©todo HTTP n√£o suportado: {method}")
    
    return response

def check_connection_exists(connection_name, workspace_id):
    """Verifica se uma connection com o nome especificado j√° existe"""
    try:
        response = make_api_request("GET", f"/workspaces/{workspace_id}/connections")
        
        if response.status_code == 200:
            connections = response.json().get("data", [])
            for connection in connections:
                if connection.get("name") == connection_name:
                    return connection.get("connectionId")
        
        return None
        
    except Exception as e:
        print(f"‚ùå Erro ao verificar connection existente: {str(e)}")
        return None

def delete_conflicting_connections(stream_name, workspace_id):
    """Deleta connections que possuem streams conflitantes"""
    deleted_count = 0
    
    try:
        # Listar todas as connections do workspace
        response = make_api_request("GET", f"/workspaces/{workspace_id}/connections")
        
        if response.status_code != 200:
            print(f"‚ùå Erro ao listar connections: {response.status_code}")
            return deleted_count
        
        connections = response.json().get("data", [])
        print(f"üîç Verificando {len(connections)} connections para conflitos com stream: {stream_name}")
        
        for connection in connections:
            connection_id = connection.get("connectionId")
            connection_name = connection.get("name", "Unknown")
            
            try:
                # Obter detalhes da connection incluindo streams
                detail_response = make_api_request("GET", f"/connections/{connection_id}")
                
                if detail_response.status_code == 200:
                    connection_detail = detail_response.json()
                    
                    # Verificar se esta connection tem o stream conflitante
                    streams = connection_detail.get("syncCatalog", {}).get("streams", [])
                    
                    has_conflicting_stream = False
                    for stream in streams:
                        stream_config = stream.get("config", {})
                        if stream_config.get("selected", False):
                            stream_stream = stream.get("stream", {})
                            current_stream_name = stream_stream.get("name", "")
                            
                            if current_stream_name == stream_name:
                                has_conflicting_stream = True
                                break
                    
                    if has_conflicting_stream:
                        print(f"üóëÔ∏è Deletando connection conflitante: {connection_name} ({connection_id})")
                        
                        # Deletar a connection
                        delete_response = make_api_request("DELETE", f"/connections/{connection_id}")
                        
                        if delete_response.status_code in [200, 204]:
                            print(f"‚úÖ Connection deletada: {connection_name}")
                            deleted_count += 1
                        else:
                            print(f"‚ùå Erro ao deletar connection {connection_name}: {delete_response.status_code}")
                    
                else:
                    print(f"‚ö†Ô∏è N√£o foi poss√≠vel obter detalhes da connection {connection_id}")
                    
            except Exception as e:
                print(f"‚ùå Erro ao processar connection {connection_id}: {str(e)}")
                continue

        if deleted_count > 0:
            print(f"üîÑ Aguardando 30 segundos para estabilizar ap√≥s {deleted_count} dele√ß√µes...")
            import time
            time.sleep(30)
        
        return deleted_count
        
    except Exception as e:
        print(f"‚ùå Erro ao deletar connections conflitantes: {str(e)}")
        return deleted_count

def generate_file_list(start_date, end_date, include_historical=True, include_recent=True):
    """
    Gera lista de arquivos baseada na regra:
    - 2000-2021: arquivos anuais (CSV)
    - 2022+: arquivos mensais (XLSX)
    """
    files = []
    
    # Processar datas
    start_dt = datetime.strptime(start_date, "%Y-%m-%d")
    
    if end_date == "auto":
        end_dt = datetime.now()
    else:
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
    
    start_year = start_dt.year
    start_month = start_dt.month
    end_year = end_dt.year
    end_month = end_dt.month
    
    # Dados hist√≥ricos: 2000-2021 (arquivos anuais)
    if include_historical:
        historical_start_year = max(2000, start_year)
        historical_end_year = min(2021, end_year)
        
        for year in range(historical_start_year, historical_end_year + 1):
            # Se for o primeiro ano, verificar se a data de in√≠cio permite incluir o ano todo
            if year == start_year and start_month > 1:
                continue  # Pula se come√ßou no meio do ano (dados anuais n√£o fazem sentido)
            
            # Se for o √∫ltimo ano, verificar se a data de fim permite incluir o ano todo
            if year == end_year and end_month < 12:
                continue  # Pula se termina no meio do ano (dados anuais n√£o fazem sentido)
                
            files.append({
                'year': year,
                'month': None,
                'filename': f'GERACAO_USINA-2_{year}.csv',
                'url': f'https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/geracao_usina_2_ho/GERACAO_USINA-2_{year}.csv',
                'stream_name': f'geracao_usina_2_{year}',
                'type': 'yearly'
            })
    
    # Dados recentes: 2022+ (arquivos mensais)
    if include_recent:
        current_year = datetime.now().year
        current_month = datetime.now().month
        
        recent_start_year = max(2022, start_year)
        recent_end_year = min(current_year, end_year)
        
        for year in range(recent_start_year, recent_end_year + 1):
            # Determinar m√™s inicial e final para este ano
            month_start = start_month if year == start_year else 1
            
            if year == end_year:
                month_end = end_month
            elif year == current_year:
                month_end = current_month  # N√£o processa meses futuros do ano atual
            else:
                month_end = 12
            
            for month in range(month_start, month_end + 1):
                files.append({
                    'year': year,
                    'month': month,
                    'filename': f'GERACAO_USINA-2_{year}_{month:02d}.xlsx',
                    'url': f'https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/geracao_usina_2_ho/GERACAO_USINA-2_{year}_{month:02d}.xlsx',
                    'stream_name': f'geracao_usina_2_{year}_{month:02d}',
                    'type': 'monthly'
                })
    
    return files

@task(dag=dag)
def check_configuration():
    """Verificar se todas as vari√°veis necess√°rias est√£o configuradas"""
    print("üîç Verificando configura√ß√£o das vari√°veis do Airflow...")
    
    required_vars = {
        "AIRBYTE_WORKSPACE_ID": "ID do workspace do Airbyte",
        "AIRBYTE_CLIENT_ID": "Client ID da aplica√ß√£o Airbyte",
        "AIRBYTE_CLIENT_SECRET": "Client Secret da aplica√ß√£o Airbyte", 
        "AIRBYTE_DESTINATION_ID": "ID do destino Snowflake no Airbyte"
    }
    
    missing_vars = []
    configured_vars = []
    
    for var_name, description in required_vars.items():
        try:
            value = Variable.get(var_name, default_var=None)
            if value:
                configured_vars.append(f"‚úÖ {var_name}: {description}")
            else:
                missing_vars.append(f"‚ùå {var_name}: {description}")
        except Exception:
            missing_vars.append(f"‚ùå {var_name}: {description}")
    
    print("\nüìã Status das vari√°veis:")
    for var in configured_vars:
        print(f"  {var}")
    for var in missing_vars:
        print(f"  {var}")
    
    if missing_vars:
        print(f"\n‚ö†Ô∏è {len(missing_vars)} vari√°vel(eis) n√£o configurada(s)")
        print("\nüìù Para configurar as vari√°veis:")
        print("1. Acesse Admin > Variables na UI do Airflow")
        print("2. Clique em '+' para criar uma nova vari√°vel")
        print("3. Adicione cada vari√°vel com seu respectivo valor")
        print("\nüîó Exemplo de valores:")
        print("- AIRBYTE_WORKSPACE_ID: 71262590-7a33-4874-8be1-d80cc8125c1c")
        print("- AIRBYTE_CLIENT_ID: seu_client_id_aqui")
        print("- AIRBYTE_CLIENT_SECRET: seu_client_secret_aqui")
        print("- AIRBYTE_DESTINATION_ID: 778daa7c-feaf-4db6-96f3-70fd645acc77")
        
        raise Exception(f"‚ùå Configura√ß√£o incompleta: {len(missing_vars)} vari√°vel(eis) ausente(s)")
    
    print(f"\n‚úÖ Todas as {len(required_vars)} vari√°veis est√£o configuradas!")
    return True

@task(dag=dag)
def create_sources_task():
    """Cria sources no Airbyte para dados hist√≥ricos e recentes"""
    try:
        context = get_current_context()
        params = context.get('params', {})
    except Exception as e:
        print(f"Erro ao obter contexto, usando par√¢metros padr√£o: {e}")
        params = {}
    
    start_date = params.get('start_date', '2025-01-01')
    end_date = params.get('end_date', 'auto')
    force_recreate = params.get('force_recreate', False)
    include_historical = params.get('include_historical', True)
    include_recent = params.get('include_recent', True)
    
    print(f"üöÄ Iniciando cria√ß√£o de sources para GERACAO_USINA_2")
    print(f"üìÖ Per√≠odo: {start_date} - {end_date}")
    print(f"üìä Incluir hist√≥ricos (2000-2021): {include_historical}")
    print(f"üìä Incluir recentes (2022+): {include_recent}")
    
    # Configura√ß√µes do Airbyte
    try:
        workspace_id = Variable.get("AIRBYTE_WORKSPACE_ID", default_var=None)
        if not workspace_id:
            raise Exception("‚ùå Vari√°vel AIRBYTE_WORKSPACE_ID deve ser configurada no Airflow")
    except Exception as e:
        print(f"‚ùå Erro ao obter workspace_id: {str(e)}")
        print("üìã Vari√°veis necess√°rias no Airflow:")
        print("   - AIRBYTE_WORKSPACE_ID: ID do workspace do Airbyte")
        print("   - AIRBYTE_CLIENT_ID: Client ID da aplica√ß√£o Airbyte")
        print("   - AIRBYTE_CLIENT_SECRET: Client Secret da aplica√ß√£o Airbyte")
        print("   - AIRBYTE_DESTINATION_ID: ID do destino Snowflake no Airbyte")
        raise
    
    # Gerar lista de arquivos
    files_to_process = generate_file_list(start_date, end_date, include_historical, include_recent)
    
    print(f"üìÅ Total de arquivos a processar: {len(files_to_process)}")
    
    # Contadores para relat√≥rio
    created_sources = 0
    skipped_sources = 0
    errors = 0
    sources_created = []
    
    for file_info in files_to_process:
        try:
            year = file_info['year']
            month = file_info['month']
            filename = file_info['filename']
            url = file_info['url']
            stream_name = file_info['stream_name']
            file_type = file_info['type']
            
            type_label = "üìÖ ANUAL" if file_type == 'yearly' else "üìÜ MENSAL"
            period_label = str(year) if file_type == 'yearly' else f"{year}-{month:02d}"
            
            print(f"\n{type_label} Processando {period_label}: {filename}")
            
            # Nome da source no Airbyte
            source_name = f"GERACAO_USINA_2_{stream_name}"
            
            # Verificar se source j√° existe
            existing_source_id = check_source_exists(source_name, workspace_id)
            
            if existing_source_id and not force_recreate:
                print(f"‚è≠Ô∏è Source j√° existe: {existing_source_id}")
                sources_created.append({
                    "file_info": file_info,
                    "source_id": existing_source_id,
                    "source_name": source_name,
                    "status": "existing"
                })
                skipped_sources += 1
                continue
            elif existing_source_id and force_recreate:
                print(f"üóëÔ∏è Deletando source existente para recriar: {existing_source_id}")
                try:
                    delete_response = make_api_request("DELETE", f"/sources/{existing_source_id}")
                    if delete_response.status_code == 204:
                        print(f"‚úÖ Source deletado com sucesso")
                    else:
                        print(f"‚ö†Ô∏è N√£o foi poss√≠vel deletar source: {delete_response.status_code}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Erro ao deletar source: {str(e)}")
            
            # Configura√ß√£o da source baseada no tipo de arquivo
            if file_type == 'yearly':
                # Arquivo CSV anual
                source_config = {
                    "sourceType": "file",
                    "configuration": {
                        "dataset_name": stream_name,
                        "format": {
                            "filetype": "csv",
                            "delimiter": ",",
                            "quote_char": '"',
                            "escape_char": "\\",
                            "encoding": "utf8",
                            "double_quote": True,
                            "newlines_in_values": False,
                            "block_size": 10000,
                            "additional_reader_options": {},
                            "advanced_options": {},
                            "skip_rows_before_header": 0,
                            "skip_rows_after_header": 0
                        },
                        "provider": {
                            "storage": "HTTPS",
                            "public_url": url,
                            "user_agent": "Airbyte/1.0"
                        }
                    }
                }
            else:
                # Arquivo XLSX mensal
                source_config = {
                    "sourceType": "file",
                    "configuration": {
                        "dataset_name": stream_name,
                        "format": {
                            "filetype": "excel",
                            "skip_rows_before_header": 0,
                            "skip_rows_after_header": 0,
                            "header_definition": {
                                "header_definition_type": "From First Row"
                            }
                        },
                        "provider": {
                            "storage": "HTTPS",
                            "public_url": url,
                            "user_agent": "Airbyte/1.0"
                        }
                    }
                }
            
            # Criar source
            source_response = make_api_request("POST", "/sources", {
                "name": source_name,
                "workspaceId": workspace_id,
                **source_config
            })
            
            if source_response.status_code == 200:
                source_data = source_response.json()
                source_id = source_data["sourceId"]
                print(f"‚úÖ Source criada: {source_name} ({source_id})")
                
                sources_created.append({
                    "file_info": file_info,
                    "source_id": source_id,
                    "source_name": source_name,
                    "status": "created"
                })
                created_sources += 1
            else:
                print(f"‚ùå Erro ao criar source: {source_response.status_code} - {source_response.text}")
                errors += 1
                
        except Exception as e:
            print(f"‚ùå Erro ao processar {filename}: {str(e)}")
            errors += 1
            continue
    
    # Relat√≥rio final
    print(f"\n" + "="*60)
    print(f"üìä RELAT√ìRIO SOURCES - GERACAO_USINA_2 v2")
    print(f"="*60)
    print(f"‚úÖ Sources criadas: {created_sources}")
    print(f"‚è≠Ô∏è Sources j√° existiam: {skipped_sources}")
    print(f"‚ùå Erros: {errors}")
    print(f"üìÅ Total processado: {len(files_to_process)}")
    print(f"="*60)
    
    return sources_created

def check_source_exists(source_name, workspace_id):
    """Verifica se um source j√° existe no workspace"""
    try:
        response = make_api_request("GET", f"/workspaces/{workspace_id}/sources")
        
        if response.status_code == 200:
            sources = response.json().get("data", [])
            for source in sources:
                if source.get("name") == source_name:
                    return source.get("sourceId")
        return None
    except Exception as e:
        print(f"‚ùå Erro ao verificar sources existentes: {str(e)}")
        return None

@task(dag=dag)
def create_connections_task(sources_created):
    """Criar connections para os sources criados"""
    if not sources_created:
        print("‚ùå Nenhum source foi processado na etapa anterior")
        return []
    
    try:
        workspace_id = Variable.get("AIRBYTE_WORKSPACE_ID", default_var=None)
        destination_id = Variable.get("AIRBYTE_DESTINATION_ID", default_var=None)
        
        if not workspace_id:
            raise Exception("‚ùå Vari√°vel AIRBYTE_WORKSPACE_ID deve ser configurada no Airflow")
        if not destination_id:
            raise Exception("‚ùå Vari√°vel AIRBYTE_DESTINATION_ID deve ser configurada no Airflow")
            
    except Exception as e:
        print(f"‚ùå Erro ao obter vari√°veis: {str(e)}")
        print("üìã Vari√°veis necess√°rias no Airflow:")
        print("   - AIRBYTE_WORKSPACE_ID: ID do workspace do Airbyte")
        print("   - AIRBYTE_DESTINATION_ID: ID do destino Snowflake no Airbyte")
        raise
    
    connections_created = []
    
    print(f"üîó Iniciando cria√ß√£o de connections...")
    print(f"üìç Destination ID: {destination_id}")
    
    for source_info in sources_created:
        source_id = source_info["source_id"]
        source_name = source_info["source_name"]
        file_info = source_info["file_info"]
        source_status = source_info.get("status", "unknown")
        
        year = file_info['year']
        month = file_info['month']
        stream_name = file_info['stream_name']
        file_type = file_info['type']
        
        period_label = str(year) if file_type == 'yearly' else f"{year}-{month:02d}"
        
        connection_name = f"GERACAO_USINA_2_{period_label}_to_Snowflake"
        
        print(f"üîÑ Processando connection para {period_label} ({source_id}) - Status: {source_status}")
        
        # Verificar se connection j√° existe
        existing_connection_id = check_connection_exists(connection_name, workspace_id)
        
        if existing_connection_id:
            print(f"‚è≠Ô∏è Connection j√° existe: {existing_connection_id}")
            connections_created.append({
                "source_info": source_info,
                "connection_id": existing_connection_id,
                "connection_name": connection_name,
                "status": "existing"
            })
            continue
        
        # Verificar conflitos com streams
        print(f"üîç Verificando conflitos para stream: {stream_name}")
        deleted_count = delete_conflicting_connections(stream_name, workspace_id)
        if deleted_count > 0:
            print(f"üóëÔ∏è Removidas {deleted_count} connections conflitantes")
        
        # Aguardar um pouco para a source ser processada
        import time
        time.sleep(2)
        
        # Obter schema da source
        schema_response = make_api_request("POST", f"/sources/{source_id}/discover_schema", {})
        
        if schema_response.status_code == 200:
            catalog = schema_response.json()["catalog"]
            print(f"üìã Schema descoberto para {source_name}")
            
            # Configurar streams para sincroniza√ß√£o
            configured_streams = []
            for stream in catalog["streams"]:
                stream_config = {
                    "stream": stream["stream"],
                    "config": {
                        "syncMode": "full_refresh",
                        "destinationSyncMode": "overwrite",
                        "selected": True,
                        "fieldSelectionEnabled": False,
                        "selectedFields": []
                    }
                }
                configured_streams.append(stream_config)
            
            # Criar connection
            connection_response = make_api_request("POST", "/connections", {
                "name": connection_name,
                "sourceId": source_id,
                "destinationId": destination_id,
                "configurations": {
                    "streams": configured_streams
                },
                "syncCatalog": {
                    "streams": configured_streams
                },
                "status": "active",
                "scheduleType": "manual"
            })
            
            if connection_response.status_code == 200:
                connection_data = connection_response.json()
                connection_id = connection_data["connectionId"]
                print(f"‚úÖ Connection criada: {connection_name} ({connection_id})")
                
                connections_created.append({
                    "source_info": source_info,
                    "connection_id": connection_id,
                    "connection_name": connection_name,
                    "status": "created"
                })
            else:
                print(f"‚ùå Erro ao criar connection: {connection_response.status_code} - {connection_response.text}")
        else:
            print(f"‚ùå Erro ao descobrir schema: {schema_response.status_code} - {schema_response.text}")
    
    print(f"\nüìä Resumo: {len(connections_created)} connections processadas")
    return connections_created

@task(dag=dag)
def trigger_initial_sync(connections_created):
    """Dispara sincroniza√ß√£o inicial para todas as connections criadas"""
    try:
        context = get_current_context()
        params = context.get("params", {})
    except Exception as e:
        print(f"Erro ao obter contexto, usando par√¢metros padr√£o: {e}")
        params = {}
    trigger_sync = params.get("trigger_sync", True)
    
    if not trigger_sync:
        print("‚è≠Ô∏è Sync autom√°tico desabilitado via par√¢metro 'trigger_sync': False")
        return []
    
    if not connections_created:
        print("‚ùå Nenhuma connection foi criada")
        return []
    
    print(f"üöÄ Disparando sincroniza√ß√£o inicial para {len(connections_created)} connections...")
    
    sync_results = []
    
    for connection_info in connections_created:
        connection_id = connection_info["connection_id"]
        connection_name = connection_info["connection_name"]
        
        print(f"üîÑ Iniciando sync para {connection_name} ({connection_id})...")
        
        try:
            # Usar endpoint correto para iniciar sync
            sync_data = {
                "connectionId": connection_id,
                "jobType": "sync"
            }
            
            response = make_api_request("POST", "/jobs", sync_data)
            
            if response.status_code in [200, 201]:
                job_info = response.json()
                job_id = job_info.get("jobId") or job_info.get("id")
                job_status = job_info.get("status", "unknown")
                print(f"‚úÖ Sync job criado: {job_id} (status: {job_status})")
                sync_results.append({
                    "connection_id": connection_id,
                    "connection_name": connection_name,
                    "job_id": job_id,
                    "status": "started",
                    "job_status": job_status
                })
            else:
                print(f"‚ùå Erro ao iniciar sync: {response.status_code}")
                print(f"Resposta: {response.text}")
                sync_results.append({
                    "connection_id": connection_id,
                    "connection_name": connection_name,
                    "status": "failed"
                })
                
        except Exception as e:
            print(f"‚ùå Exce√ß√£o ao iniciar sync: {str(e)}")
            sync_results.append({
                "connection_id": connection_id,
                "connection_name": connection_name,
                "status": "error",
                "error": str(e)
            })
    
    successful_syncs = len([s for s in sync_results if s.get('status') == 'started'])
    print(f"\nüéâ Resumo: {successful_syncs} sincroniza√ß√µes iniciadas com sucesso!")
    
    return sync_results

# Definir as tasks e depend√™ncias
config_check = check_configuration()
sources_created = create_sources_task()
connections_created = create_connections_task(sources_created)
sync_results = trigger_initial_sync(connections_created)

# Definir depend√™ncias explicitamente
config_check >> sources_created >> connections_created >> sync_results